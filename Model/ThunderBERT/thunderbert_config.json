{
    "attention_probs_dropout_prob": 0.2,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.2,
    "hidden_size": 32,
    "bucket_list": [[0, 42], [43, 1182], [1183, 9168], [9169, 39798], [39799, 125518], [125519, 332615], [332616, 882358], [882359, 11649431]],
    "factor_list": [32, 20, 13, 8, 15, 4, 3, 2],
    "intermediate_size": 32,
    "initializer_range": 0.02,
    "max_position_embeddings": 200,
    "num_attention_heads": 2,
    "num_hidden_layers": 8,
    "type_vocab_size": 2,
    "vocab_size": 11649431
}